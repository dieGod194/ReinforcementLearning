{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Install Gymnasium\n",
        "%pip install -q -U gymnasium swig\n",
        "%pip install -q -U gymnasium[classic_control,box2d,atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s73587gk8eKG",
        "outputId": "7c1e1de6-64b8-4ccd-8d90-19895d03e0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "metadata": {
        "id": "PJ84C9h-4l_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8Q6zX1u3hSM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "8268a044-2fcf-448d-c455-f4768575d9fb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'cartpole.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ec359117399c>\u001b[0m in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m#run(is_training=True, render=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-ec359117399c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(is_training, render)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvel_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mang_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mang_vel_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# init a 11x11x11x11x2 array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cartpole.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Guarda los valores para que cuando ya no esté entrenando, sepa como moverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cartpole.pkl'"
          ]
        }
      ],
      "source": [
        "def run(is_training=True, render=False):\n",
        "\n",
        "    env = gym.make('CartPole-v1', render_mode='human' if render else None)\n",
        "\n",
        "    pos_space = np.linspace(-2.4, 2.4, 10)\n",
        "    vel_space = np.linspace(-4, 4, 10)\n",
        "    ang_space = np.linspace(-.2095, .2095, 10)\n",
        "    ang_vel_space = np.linspace(-4, 4, 10)\n",
        "\n",
        "    if(is_training):\n",
        "        q = np.zeros((len(pos_space)+1, len(vel_space)+1, len(ang_space)+1, len(ang_vel_space)+1, env.action_space.n))\n",
        "    else:\n",
        "        f = open('cartpole.pkl', 'rb')\n",
        "        q = pickle.load(f)\n",
        "        f.close()\n",
        "\n",
        "    learning_rate_a = 0.1\n",
        "    discount_factor_g = 0.99\n",
        "\n",
        "    epsilon = 1\n",
        "    epsilon_decay_rate = 0.00001\n",
        "    rng = np.random.default_rng()\n",
        "\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    i = 0\n",
        "\n",
        "\n",
        "    while(True):\n",
        "\n",
        "        state = env.reset()[0]\n",
        "        state_p = np.digitize(state[0], pos_space)\n",
        "        state_v = np.digitize(state[1], vel_space)\n",
        "        state_a = np.digitize(state[2], ang_space)\n",
        "        state_av = np.digitize(state[3], ang_vel_space)\n",
        "\n",
        "        terminated = False\n",
        "\n",
        "        rewards=0\n",
        "\n",
        "        while(not terminated and rewards < 10000):\n",
        "\n",
        "            if is_training and rng.random() < epsilon:\n",
        "\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q[state_p, state_v, state_a, state_av, :])\n",
        "\n",
        "            new_state,reward,terminated,_,_ = env.step(action)\n",
        "            new_state_p = np.digitize(new_state[0], pos_space)\n",
        "            new_state_v = np.digitize(new_state[1], vel_space)\n",
        "            new_state_a = np.digitize(new_state[2], ang_space)\n",
        "            new_state_av= np.digitize(new_state[3], ang_vel_space)\n",
        "\n",
        "            if is_training:\n",
        "                q[state_p, state_v, state_a, state_av, action] = q[state_p, state_v, state_a, state_av, action] + learning_rate_a * (\n",
        "                    reward + discount_factor_g*np.max(q[new_state_p, new_state_v, new_state_a, new_state_av,:]) - q[state_p, state_v, state_a, state_av, action]\n",
        "                )\n",
        "\n",
        "            state = new_state\n",
        "            state_p = new_state_p\n",
        "            state_v = new_state_v\n",
        "            state_a = new_state_a\n",
        "            state_av= new_state_av\n",
        "\n",
        "            rewards+=reward\n",
        "            if not is_training and rewards%100==0:\n",
        "                print(f'Episode: {i}  Rewards: {rewards}')\n",
        "\n",
        "        rewards_per_episode.append(rewards)\n",
        "        mean_rewards = np.mean(rewards_per_episode[len(rewards_per_episode)-100:])\n",
        "\n",
        "        if is_training and i%100==0:\n",
        "            print(f'Episode: {i} {rewards}  Epsilon: {epsilon:0.2f}  Mean Rewards {mean_rewards:0.1f}')\n",
        "\n",
        "        if mean_rewards>1000:\n",
        "            break\n",
        "\n",
        "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "\n",
        "        i+=1\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "    if is_training:\n",
        "        f = open('cartpole.pkl','wb')\n",
        "        pickle.dump(q, f)\n",
        "        f.close()\n",
        "\n",
        "    mean_rewards = []\n",
        "    for t in range(i):\n",
        "        mean_rewards.append(np.mean(rewards_per_episode[max(0, t-100):(t+1)]))\n",
        "    plt.plot(mean_rewards)\n",
        "    plt.savefig(f'cartpole.png')\n",
        "if __name__ == '__main__':\n",
        "    #run(is_training=True, render=False) Activate when training\n",
        "\n",
        "    run(is_training=False, render=True)"
      ]
    }
  ]
}